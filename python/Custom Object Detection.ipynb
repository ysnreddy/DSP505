{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr3q-gvm3cI8"
   },
   "source": [
    "In this project, we used [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) library to train a custom object detection model capable of detecting salads within images on a mobile device. \n",
    "\n",
    "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with own custom dataset reduces the amount of training data required and will shorten the training time.\n",
    "\n",
    "We used the publicly available *Salads* dataset, which was created from the [Open Images Dataset V4](https://storage.googleapis.com/openimages/web/index.html).\n",
    "\n",
    "Each image in the dataset \bcontains objects labeled as one of the following classes: \n",
    "* Baked Good\n",
    "* Cheese\n",
    "* Salad\n",
    "* Seafood\n",
    "* Tomato\n",
    "\n",
    "The dataset contains the bounding-boxes specifying where each object locates, together with the object's label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcLF2PKkSbV3"
   },
   "source": [
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vvAObmTqglq"
   },
   "source": [
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qhl8lqVamEty"
   },
   "outputs": [],
   "source": [
    "!pip install -q tflite-model-maker\n",
    "!pip install -q pycocotools\n",
    "!pip install -q tflite-support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6lRhVK9Q_0U"
   },
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XtxiUeZEiXpt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the dataset\n",
    "\n",
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"unauthorized\")\n",
    "project = rf.workspace(\"gw8717navercom\").project(\"salad-data\")\n",
    "dataset = project.version(1).download(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRd13bfetO7B"
   },
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "The *Salads* dataset is available at:\n",
    " `gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv.` \n",
    "\n",
    "It contains 175 images for training, 25 images for validation, and 25 images for testing. The dataset has five classes: `Salad`, `Seafood`, `Tomato`, `Baked goods`, `Cheese`.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The dataset is provided in CSV format:\n",
    "```\n",
    "TRAINING,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Salad,0.0,0.0954,,,0.977,0.957,,\n",
    "VALIDATION,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Seafood,0.0154,0.1538,,,1.0,0.802,,\n",
    "TEST,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Tomato,0.0,0.655,,,0.231,0.839,,\n",
    "```\n",
    "\n",
    "* Each row corresponds to an object localized inside a larger image, with each object specifically designated as test, train, or validation data.\n",
    "* The three lines included here indicate **three distinct objects located inside the same image** available at `gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg`.\n",
    "* Each row has a different label: `Salad`, `Seafood`, `Tomato`, etc.\n",
    "* Bounding boxes are specified for each image using the top left and bottom right vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5M8iuydhVae"
   },
   "source": [
    "## Train your salad detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xushUyZXqP59"
   },
   "source": [
    "#### Object detection model archiecture.\n",
    "\n",
    "We use the EfficientDet-Lite2 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture. \n",
    "\n",
    "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
    "\n",
    "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
    "|--------------------|-----------|---------------|----------------------|\n",
    "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
    "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
    "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
    "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
    "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
    "\n",
    "<i> * Size of the integer quantized models. <br/>\n",
    "** Latency measured on Pixel 4 using 4 threads on CPU [Ref: Google TF Lite Documentaion]. <br/>\n",
    "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
    "</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CtdZ-JDwMimd"
   },
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5U-A3tw6Y27"
   },
   "source": [
    "**Step 2. Load the dataset.**\n",
    "\n",
    "Model Maker will take input data in the CSV format. Use the `ObjectDetectorDataloader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
    "\n",
    "\n",
    "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. We can specify a local CSV file on your computer, and Model Maker will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HD5BvzWe6YKa"
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uZkLR6N6gDR"
   },
   "source": [
    "**Step 3. Train the TensorFlow model with the training data.**\n",
    "\n",
    "* The EfficientDet-Lite0 model uses `epochs = 50` by default.\n",
    "* Set `batch_size = 8` here so we will see that it takes 21 steps to go through the 175 images in the training dataset. \n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwlYdTcg63xy",
    "outputId": "c29fc0d3-a978-49e3-ce28-9eb78d897ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 79s 2s/step - det_loss: 1.7540 - cls_loss: 1.1316 - box_loss: 0.0124 - reg_l2_loss: 0.0764 - loss: 1.8305 - learning_rate: 0.0090 - gradient_norm: 0.7342 - val_det_loss: 1.6683 - val_cls_loss: 1.1064 - val_box_loss: 0.0112 - val_reg_l2_loss: 0.0764 - val_loss: 1.7447\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 28s 1s/step - det_loss: 1.5948 - cls_loss: 1.0771 - box_loss: 0.0104 - reg_l2_loss: 0.0764 - loss: 1.6713 - learning_rate: 0.0100 - gradient_norm: 1.0794 - val_det_loss: 1.4721 - val_cls_loss: 0.9567 - val_box_loss: 0.0103 - val_reg_l2_loss: 0.0764 - val_loss: 1.5485\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 1.3778 - cls_loss: 0.9399 - box_loss: 0.0088 - reg_l2_loss: 0.0764 - loss: 1.4542 - learning_rate: 0.0099 - gradient_norm: 1.9733 - val_det_loss: 1.2991 - val_cls_loss: 0.8070 - val_box_loss: 0.0098 - val_reg_l2_loss: 0.0764 - val_loss: 1.3755\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 1.1941 - cls_loss: 0.8000 - box_loss: 0.0079 - reg_l2_loss: 0.0765 - loss: 1.2705 - learning_rate: 0.0099 - gradient_norm: 2.2215 - val_det_loss: 1.2332 - val_cls_loss: 0.7579 - val_box_loss: 0.0095 - val_reg_l2_loss: 0.0765 - val_loss: 1.3096\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 42s 2s/step - det_loss: 1.0751 - cls_loss: 0.7224 - box_loss: 0.0071 - reg_l2_loss: 0.0765 - loss: 1.1516 - learning_rate: 0.0098 - gradient_norm: 2.0067 - val_det_loss: 1.1441 - val_cls_loss: 0.6791 - val_box_loss: 0.0093 - val_reg_l2_loss: 0.0765 - val_loss: 1.2206\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 27s 1s/step - det_loss: 0.9900 - cls_loss: 0.6598 - box_loss: 0.0066 - reg_l2_loss: 0.0765 - loss: 1.0665 - learning_rate: 0.0097 - gradient_norm: 2.1257 - val_det_loss: 1.1280 - val_cls_loss: 0.6906 - val_box_loss: 0.0087 - val_reg_l2_loss: 0.0765 - val_loss: 1.2045\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.9325 - cls_loss: 0.6292 - box_loss: 0.0061 - reg_l2_loss: 0.0765 - loss: 1.0090 - learning_rate: 0.0096 - gradient_norm: 2.2243 - val_det_loss: 1.0613 - val_cls_loss: 0.6587 - val_box_loss: 0.0081 - val_reg_l2_loss: 0.0765 - val_loss: 1.1378\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.8897 - cls_loss: 0.6043 - box_loss: 0.0057 - reg_l2_loss: 0.0765 - loss: 0.9662 - learning_rate: 0.0094 - gradient_norm: 2.8098 - val_det_loss: 1.0362 - val_cls_loss: 0.6417 - val_box_loss: 0.0079 - val_reg_l2_loss: 0.0765 - val_loss: 1.1127\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.8586 - cls_loss: 0.5871 - box_loss: 0.0054 - reg_l2_loss: 0.0765 - loss: 0.9352 - learning_rate: 0.0093 - gradient_norm: 2.4246 - val_det_loss: 1.0614 - val_cls_loss: 0.6745 - val_box_loss: 0.0077 - val_reg_l2_loss: 0.0765 - val_loss: 1.1379\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 39s 2s/step - det_loss: 0.8319 - cls_loss: 0.5635 - box_loss: 0.0054 - reg_l2_loss: 0.0765 - loss: 0.9085 - learning_rate: 0.0091 - gradient_norm: 2.4800 - val_det_loss: 1.0574 - val_cls_loss: 0.7111 - val_box_loss: 0.0069 - val_reg_l2_loss: 0.0766 - val_loss: 1.1339\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 28s 1s/step - det_loss: 0.8193 - cls_loss: 0.5535 - box_loss: 0.0053 - reg_l2_loss: 0.0766 - loss: 0.8959 - learning_rate: 0.0089 - gradient_norm: 2.4925 - val_det_loss: 0.9448 - val_cls_loss: 0.5909 - val_box_loss: 0.0071 - val_reg_l2_loss: 0.0766 - val_loss: 1.0214\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 0.7680 - cls_loss: 0.5248 - box_loss: 0.0049 - reg_l2_loss: 0.0766 - loss: 0.8446 - learning_rate: 0.0087 - gradient_norm: 2.3692 - val_det_loss: 0.9371 - val_cls_loss: 0.6060 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0766 - val_loss: 1.0137\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.7665 - cls_loss: 0.5126 - box_loss: 0.0051 - reg_l2_loss: 0.0766 - loss: 0.8431 - learning_rate: 0.0085 - gradient_norm: 2.4437 - val_det_loss: 0.9562 - val_cls_loss: 0.6254 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0766 - val_loss: 1.0328\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.7569 - cls_loss: 0.5062 - box_loss: 0.0050 - reg_l2_loss: 0.0766 - loss: 0.8335 - learning_rate: 0.0082 - gradient_norm: 2.5508 - val_det_loss: 0.9951 - val_cls_loss: 0.6652 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0766 - val_loss: 1.0717\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 37s 2s/step - det_loss: 0.7263 - cls_loss: 0.4874 - box_loss: 0.0048 - reg_l2_loss: 0.0766 - loss: 0.8029 - learning_rate: 0.0080 - gradient_norm: 2.5737 - val_det_loss: 1.0902 - val_cls_loss: 0.7499 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0766 - val_loss: 1.1668\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 29s 1s/step - det_loss: 0.7080 - cls_loss: 0.4727 - box_loss: 0.0047 - reg_l2_loss: 0.0766 - loss: 0.7846 - learning_rate: 0.0077 - gradient_norm: 2.5114 - val_det_loss: 1.1366 - val_cls_loss: 0.7490 - val_box_loss: 0.0078 - val_reg_l2_loss: 0.0766 - val_loss: 1.2132\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 0.7157 - cls_loss: 0.4860 - box_loss: 0.0046 - reg_l2_loss: 0.0766 - loss: 0.7923 - learning_rate: 0.0075 - gradient_norm: 2.9363 - val_det_loss: 0.9515 - val_cls_loss: 0.6181 - val_box_loss: 0.0067 - val_reg_l2_loss: 0.0766 - val_loss: 1.0282\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.6736 - cls_loss: 0.4507 - box_loss: 0.0045 - reg_l2_loss: 0.0766 - loss: 0.7502 - learning_rate: 0.0072 - gradient_norm: 2.5948 - val_det_loss: 0.9162 - val_cls_loss: 0.5778 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0766 - val_loss: 0.9929\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.6756 - cls_loss: 0.4548 - box_loss: 0.0044 - reg_l2_loss: 0.0766 - loss: 0.7523 - learning_rate: 0.0069 - gradient_norm: 2.7629 - val_det_loss: 0.9802 - val_cls_loss: 0.6367 - val_box_loss: 0.0069 - val_reg_l2_loss: 0.0767 - val_loss: 1.0569\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 39s 2s/step - det_loss: 0.6782 - cls_loss: 0.4609 - box_loss: 0.0043 - reg_l2_loss: 0.0767 - loss: 0.7548 - learning_rate: 0.0066 - gradient_norm: 3.0323 - val_det_loss: 0.9524 - val_cls_loss: 0.6240 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0767 - val_loss: 1.0290\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 27s 1s/step - det_loss: 0.6510 - cls_loss: 0.4410 - box_loss: 0.0042 - reg_l2_loss: 0.0767 - loss: 0.7276 - learning_rate: 0.0063 - gradient_norm: 2.7220 - val_det_loss: 0.9979 - val_cls_loss: 0.6741 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0767 - val_loss: 1.0745\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.6324 - cls_loss: 0.4250 - box_loss: 0.0041 - reg_l2_loss: 0.0767 - loss: 0.7090 - learning_rate: 0.0060 - gradient_norm: 2.7191 - val_det_loss: 0.9098 - val_cls_loss: 0.5937 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0767 - val_loss: 0.9865\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.6389 - cls_loss: 0.4340 - box_loss: 0.0041 - reg_l2_loss: 0.0767 - loss: 0.7156 - learning_rate: 0.0056 - gradient_norm: 2.5071 - val_det_loss: 0.9082 - val_cls_loss: 0.5809 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0767 - val_loss: 0.9849\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 33s 2s/step - det_loss: 0.6194 - cls_loss: 0.4155 - box_loss: 0.0041 - reg_l2_loss: 0.0767 - loss: 0.6961 - learning_rate: 0.0053 - gradient_norm: 2.6125 - val_det_loss: 0.9381 - val_cls_loss: 0.5992 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0767 - val_loss: 1.0148\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 36s 2s/step - det_loss: 0.6236 - cls_loss: 0.4219 - box_loss: 0.0040 - reg_l2_loss: 0.0767 - loss: 0.7002 - learning_rate: 0.0050 - gradient_norm: 2.7780 - val_det_loss: 0.9034 - val_cls_loss: 0.5817 - val_box_loss: 0.0064 - val_reg_l2_loss: 0.0767 - val_loss: 0.9801\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 27s 1s/step - det_loss: 0.5946 - cls_loss: 0.4047 - box_loss: 0.0038 - reg_l2_loss: 0.0767 - loss: 0.6713 - learning_rate: 0.0047 - gradient_norm: 2.6760 - val_det_loss: 0.9432 - val_cls_loss: 0.6089 - val_box_loss: 0.0067 - val_reg_l2_loss: 0.0767 - val_loss: 1.0199\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.5874 - cls_loss: 0.3964 - box_loss: 0.0038 - reg_l2_loss: 0.0767 - loss: 0.6641 - learning_rate: 0.0044 - gradient_norm: 2.5312 - val_det_loss: 0.8899 - val_cls_loss: 0.5683 - val_box_loss: 0.0064 - val_reg_l2_loss: 0.0767 - val_loss: 0.9666\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.5868 - cls_loss: 0.3929 - box_loss: 0.0039 - reg_l2_loss: 0.0767 - loss: 0.6635 - learning_rate: 0.0040 - gradient_norm: 2.5917 - val_det_loss: 0.8896 - val_cls_loss: 0.5789 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0767 - val_loss: 0.9663\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.5709 - cls_loss: 0.3776 - box_loss: 0.0039 - reg_l2_loss: 0.0767 - loss: 0.6476 - learning_rate: 0.0037 - gradient_norm: 2.8046 - val_det_loss: 0.8352 - val_cls_loss: 0.5339 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0767 - val_loss: 0.9119\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 39s 2s/step - det_loss: 0.5810 - cls_loss: 0.3905 - box_loss: 0.0038 - reg_l2_loss: 0.0767 - loss: 0.6577 - learning_rate: 0.0034 - gradient_norm: 2.9586 - val_det_loss: 0.8433 - val_cls_loss: 0.5326 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0767 - val_loss: 0.9200\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 26s 1s/step - det_loss: 0.5559 - cls_loss: 0.3722 - box_loss: 0.0037 - reg_l2_loss: 0.0767 - loss: 0.6326 - learning_rate: 0.0031 - gradient_norm: 2.6502 - val_det_loss: 0.8459 - val_cls_loss: 0.5347 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0767 - val_loss: 0.9226\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 0.5849 - cls_loss: 0.3889 - box_loss: 0.0039 - reg_l2_loss: 0.0767 - loss: 0.6616 - learning_rate: 0.0028 - gradient_norm: 2.8577 - val_det_loss: 0.8190 - val_cls_loss: 0.5116 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0767 - val_loss: 0.8957\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.5533 - cls_loss: 0.3728 - box_loss: 0.0036 - reg_l2_loss: 0.0767 - loss: 0.6300 - learning_rate: 0.0025 - gradient_norm: 2.5942 - val_det_loss: 0.8221 - val_cls_loss: 0.5154 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0767 - val_loss: 0.8988\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.5453 - cls_loss: 0.3630 - box_loss: 0.0036 - reg_l2_loss: 0.0767 - loss: 0.6220 - learning_rate: 0.0023 - gradient_norm: 2.6230 - val_det_loss: 0.8278 - val_cls_loss: 0.5262 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0767 - val_loss: 0.9045\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 39s 2s/step - det_loss: 0.5327 - cls_loss: 0.3547 - box_loss: 0.0036 - reg_l2_loss: 0.0767 - loss: 0.6094 - learning_rate: 0.0020 - gradient_norm: 2.7613 - val_det_loss: 0.8254 - val_cls_loss: 0.5259 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0767 - val_loss: 0.9021\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 27s 1s/step - det_loss: 0.5838 - cls_loss: 0.3864 - box_loss: 0.0039 - reg_l2_loss: 0.0767 - loss: 0.6605 - learning_rate: 0.0018 - gradient_norm: 2.9514 - val_det_loss: 0.8244 - val_cls_loss: 0.5205 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0767 - val_loss: 0.9011\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 0.5529 - cls_loss: 0.3711 - box_loss: 0.0036 - reg_l2_loss: 0.0767 - loss: 0.6296 - learning_rate: 0.0015 - gradient_norm: 2.8103 - val_det_loss: 0.8264 - val_cls_loss: 0.5206 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0767 - val_loss: 0.9031\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 0.5210 - cls_loss: 0.3460 - box_loss: 0.0035 - reg_l2_loss: 0.0767 - loss: 0.5977 - learning_rate: 0.0013 - gradient_norm: 2.5456 - val_det_loss: 0.8119 - val_cls_loss: 0.5050 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0767 - val_loss: 0.8886\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 31s 2s/step - det_loss: 0.5340 - cls_loss: 0.3630 - box_loss: 0.0034 - reg_l2_loss: 0.0767 - loss: 0.6107 - learning_rate: 0.0011 - gradient_norm: 2.9200 - val_det_loss: 0.8013 - val_cls_loss: 0.4987 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0767 - val_loss: 0.8780\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 36s 2s/step - det_loss: 0.5393 - cls_loss: 0.3684 - box_loss: 0.0034 - reg_l2_loss: 0.0767 - loss: 0.6160 - learning_rate: 9.0029e-04 - gradient_norm: 2.6860 - val_det_loss: 0.7984 - val_cls_loss: 0.5011 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8751\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 28s 1s/step - det_loss: 0.5341 - cls_loss: 0.3605 - box_loss: 0.0035 - reg_l2_loss: 0.0767 - loss: 0.6109 - learning_rate: 7.2543e-04 - gradient_norm: 2.8528 - val_det_loss: 0.7987 - val_cls_loss: 0.5022 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8754\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.5278 - cls_loss: 0.3543 - box_loss: 0.0035 - reg_l2_loss: 0.0767 - loss: 0.6045 - learning_rate: 5.6814e-04 - gradient_norm: 2.5555 - val_det_loss: 0.8040 - val_cls_loss: 0.5077 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8807\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.5159 - cls_loss: 0.3495 - box_loss: 0.0033 - reg_l2_loss: 0.0767 - loss: 0.5926 - learning_rate: 4.2906e-04 - gradient_norm: 2.5067 - val_det_loss: 0.8016 - val_cls_loss: 0.5053 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8783\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 30s 1s/step - det_loss: 0.5292 - cls_loss: 0.3576 - box_loss: 0.0034 - reg_l2_loss: 0.0767 - loss: 0.6059 - learning_rate: 3.0876e-04 - gradient_norm: 2.7049 - val_det_loss: 0.8089 - val_cls_loss: 0.5124 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8856\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 37s 2s/step - det_loss: 0.5185 - cls_loss: 0.3514 - box_loss: 0.0033 - reg_l2_loss: 0.0767 - loss: 0.5952 - learning_rate: 2.0774e-04 - gradient_norm: 2.6082 - val_det_loss: 0.8084 - val_cls_loss: 0.5112 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8851\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 29s 1s/step - det_loss: 0.5213 - cls_loss: 0.3495 - box_loss: 0.0034 - reg_l2_loss: 0.0767 - loss: 0.5980 - learning_rate: 1.2641e-04 - gradient_norm: 2.5413 - val_det_loss: 0.8121 - val_cls_loss: 0.5145 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0767 - val_loss: 0.8888\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 29s 1s/step - det_loss: 0.5213 - cls_loss: 0.3549 - box_loss: 0.0033 - reg_l2_loss: 0.0767 - loss: 0.5980 - learning_rate: 6.5107e-05 - gradient_norm: 2.6655 - val_det_loss: 0.8092 - val_cls_loss: 0.5122 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8859\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.5347 - cls_loss: 0.3525 - box_loss: 0.0036 - reg_l2_loss: 0.0767 - loss: 0.6114 - learning_rate: 2.4083e-05 - gradient_norm: 2.7069 - val_det_loss: 0.8091 - val_cls_loss: 0.5121 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8858\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 32s 2s/step - det_loss: 0.5189 - cls_loss: 0.3561 - box_loss: 0.0033 - reg_l2_loss: 0.0767 - loss: 0.5956 - learning_rate: 3.5074e-06 - gradient_norm: 2.6314 - val_det_loss: 0.8115 - val_cls_loss: 0.5141 - val_box_loss: 0.0059 - val_reg_l2_loss: 0.0767 - val_loss: 0.8882\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 36s 2s/step - det_loss: 0.5440 - cls_loss: 0.3595 - box_loss: 0.0037 - reg_l2_loss: 0.0767 - loss: 0.6207 - learning_rate: 3.4629e-06 - gradient_norm: 2.7541 - val_det_loss: 0.8113 - val_cls_loss: 0.5137 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0767 - val_loss: 0.8880\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BzCHLWJ6h7q"
   },
   "source": [
    "**Step 4. Evaluate the model with the test data.**\n",
    "\n",
    "After training the object detection model using the images in the training dataset, we use the remaining 25 images in the test dataset to evaluate how the model performs against new data.\n",
    "\n",
    "As the default batch size is 64, it will take 1 step to go through the 25 images in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xmnl6Yy7ARn",
    "outputId": "172af657-2fd5-4617-80e9-39e5a469632a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 9s 9s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.23125926,\n",
       " 'AP50': 0.39211,\n",
       " 'AP75': 0.22839592,\n",
       " 'AP_/Baked Goods': 0.056009296,\n",
       " 'AP_/Cheese': 0.25100884,\n",
       " 'AP_/Salad': 0.59339285,\n",
       " 'AP_/Seafood': 0.019524494,\n",
       " 'AP_/Tomato': 0.23636083,\n",
       " 'APl': 0.23417331,\n",
       " 'APm': 0.2912734,\n",
       " 'APs': -1.0,\n",
       " 'ARl': 0.42843384,\n",
       " 'ARm': 0.49166667,\n",
       " 'ARmax1': 0.17963235,\n",
       " 'ARmax10': 0.37321636,\n",
       " 'ARmax100': 0.43112075,\n",
       " 'ARs': -1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgCDMe0e6jlT"
   },
   "source": [
    "**Step 5.  Export as a TensorFlow Lite model.**\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Hm_UULdW7A9T"
   },
   "outputs": [],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UygYErfCD5m3"
   },
   "source": [
    "**Step 6.  Evaluate the TensorFlow Lite model.**\n",
    "\n",
    "Several factors can affect the model accuracy when exporting to TFLite:\n",
    "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop. \n",
    "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
    "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
    "\n",
    "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHYDWcljr6jq",
    "outputId": "30cc03eb-a2e9-4b0f-e665-eb354bed940f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 191s 8s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.19890182,\n",
       " 'AP50': 0.34420288,\n",
       " 'AP75': 0.20404717,\n",
       " 'AP_/Baked Goods': 0.035757806,\n",
       " 'AP_/Cheese': 0.20603977,\n",
       " 'AP_/Salad': 0.536264,\n",
       " 'AP_/Seafood': 0.0005638154,\n",
       " 'AP_/Tomato': 0.2158837,\n",
       " 'APl': 0.20219831,\n",
       " 'APm': 0.28807756,\n",
       " 'APs': -1.0,\n",
       " 'ARl': 0.28367373,\n",
       " 'ARm': 0.39166668,\n",
       " 'ARmax1': 0.14983957,\n",
       " 'ARmax10': 0.2678721,\n",
       " 'ARmax100': 0.28562945,\n",
       " 'ARs': -1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_tflite('model.tflite', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVxaf3x_7OfB"
   },
   "source": [
    "In the final step , we used the [ObjectDetector API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) to integrate the model into the Android app."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train a salad detector with TFLite Model Maker",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
